---
title: "Synthetic Sonic Log Generation"
date: 2020-07-17
tags: [machine learning]
header:
  image: "/images/02_well/profiles_cut.jpg "
excerpt: "Machine Learning"
mathjax: "true"
---

# Introduction
## Background
Well logs are interpreted/processed to estimate the in-situ petrophysical and geomechanical properties, which is essential for subsurface characterization. Various types of logs exist, and each provides distinct information about subsurface properties. Certain well logs, like gamma ray (GR), resistivity, density, and neutron logs, are considered as “easy-to-acquire” conventional well logs that are run in most of the wells. Other well logs, like nuclear magnetic resonance, dielectric dispersion, elemental spectroscopy, and sometimes sonic logs, are only run in limited number of wells.
## Problem Statement
Compressional travel-time (DTC) and shear travel-time (DTS) logs are not acquired in all the wells drilled in a field due to financial or operational constraints. Under such circumstances, machine learning techniques can be used to predict DTC and DTS logs to improve subsurface characterization. The goal of the “SPWLA’s 1st Petrophysical Data-Driven Analytics Contest” is to develop data-driven models by processing “easy-to-acquire” conventional logs from Well #1, and use the data-driven models to generate synthetic compressional and shear travel-time logs (DTC and DTS, respectively) in Well #2. A robust data-driven model for the desired sonic-log synthesis will result in low prediction errors, which can be quantified in terms of Root Mean Squared Error by comparing the synthesized and the original DTC and DTS logs.

You are provided with two datasets: train.csv and test.csv. You need to build a generalizable data-driven models using train dataset. Following that, you will deploy the newly developed data-driven models on test dataset to predict DTS and DTC logs. The data-driven model should use feature sets derived from the following 7 logs: Caliper, Neutron, Gamma Ray, Deep Resistivity, Medium Resistivity, Photo-electric factor and density. The data-driven model should synthesize two target logs: DTC and DTS logs.

The predicted values should be in the same format as sample_submission.csv, and submit together with your notebook for evaluation.

## Data Description
The training data includes 7 key features and 2 targets. The test data has all features used in the train dataset, except the two sonic curves DTC and DTS. All values equals to -999 are marked as missing value.
CAL - Caliper, unit in Inch,<br>
CNC - Neutron, unit in dec,<br>
GR - Gamma Ray, unit in API,<br>
HRD - Deep Resisitivity, unit in Ohm per meter,<br>
HRM - Medium Resistivity, unit in Ohm per meter,<br>
PE - Photo-electric Factor, unit in Barn,<br>
ZDEN - Density, unit in Gram per cubit meter,<br>
DTC - Compressional Travel-time, unit in nanosecond per foot,<br>
DTS - Shear Travel-time, unit in nanosecond per foot<br>

## Evaluation Metrics
The results were evaluated by the metric Root Mean Squared Error. The RMSE is calculated as:<br>
$$ ϵ=\sum_i \sqrt{ \sum_n (y_p - y_t)^2 /n } $$<br>
Where:
y_p is the predicted curve for DTC and DTS
y_t is the true value for evaluation.
DTC and DTS are in the same weight during the evaluation

# EDA and Data Cleaning
The first glimpse on the data is to plot the vertical profiles of each feature and the targets in the train dataset. The key observations are that the target DTS is relatively larger in the depth index range 0-5000, where CNC is also found to be less stable.  HDR and HDM values are quite similar. We need to pay attention on these highly correlated variables if we want to use linear regression.

Match the data feature field:
```python
def plot_profile(data):
    plt.figure(figsize = (25,25))
    plt.title('Well Log Variable Profiles')
    y_tmp = range(data.shape[0])
    var = ['CAL','GR', 'CNC', 'ZDEN', 'PE', 'HRD','DTC']
    scales = [[0,35],[-50, 250], [-0.15, 1], [1, 4], [-5,30], [0.2, 5], [30,500]]
    for i in range(len(var)):
        plt.subplot(171+i)
        plt.plot(var[i], y_tmp, data = data)
        plt.xlim(scales[i])
        plt.ylim([0,33000])
        if var[i] == 'HRD':
            plt.plot('HRM', y_tmp, data = data, c = 'red', label = 'HRM')
        elif var[i] == 'DTC':
            plt.plot('DTS', y_tmp, data = data, c = 'red', label = 'HRM')
        plt.gca().invert_yaxis()
        plt.title(var[i])
plot_profile(df1)
```
*feature vertical profiles* <br>
![alt]({{ site.url }}{{ site.baseurl }}/images/02_well/profiles.jpg)

After removing the nan values and the outliers, the histogram of each feature was also examined. I found the HDM and HDR values are pretty skewed. Thus a log transformation is applied. The pairplot was used to check the correlations between the features in train dataset. Among the features, HRM and HRD are strongly correlated, and some obscure relationship can be observed between GR and CNC.
```python
df_plot=df1.copy()
df_plot['CNC'][df_plot['CNC']>0.6]=np.nan
df_plot['CNC'][df_plot['CNC']<-0.15]=np.nan
df_plot['GR'][df_plot['GR']>250]=150
#df_plot['HRD_log'][df_plot['HRD_log']>150] = np.nan
#df_plot['HRM_log'][df_plot['HRM_log']>150] = np.nan
df_plot.dropna(axis=0, inplace=True)
g=sb.pairplot(df_plot[['CAL','GR', 'CNC', 'ZDEN', 'PE', 'HRD_log','HRM_log','DTC','DTS']])
```
*pairplot* <br>
![alt]({{ site.url }}{{ site.baseurl }}/images/02_well/pairplot.jpg)

In this pairplot, we also found out that some features are strong indicators of the targets. For example, DTC and DTS are obviously correlated with CAL, GR and CNC. To further investigate the predicting powers of each feature, I calculated the correlation coefficients between the features and targets. CNC is a strong predictor with correlation coefficients near 0.9.
```python
fig, ax = plt.subplots(figsize = (25,15))
cm_df = sb.heatmap(df_plot[['CAL','GR', 'CNC', 'ZDEN', 'PE', 'HRD_log','HRM_log','DTC','DTS']].corr(), annot = True, fmt = ".2f", cmap = 'coolwarm', ax = ax)
```
*correlation* <br>
![alt]({{ site.url }}{{ site.baseurl }}/images/02_well/corr.jpg)

# Model Building and Validation
I used the MinMaxScaler transform to scale the feature values in the dataset. I started the predictive model building from the simple linear regression, and tried the liner regression with regularization (elasticnet), random forest, gradient boosting trees and neural network. The hyperopt package was used to fine tune the parameters. In the end, the predict model based on the gradient boosting trees provided the best RMSE (17.11).
```python
xgb_dtc = xgb.XGBRegressor(random_state=42)
space ={
        'n_estimators' : scope.int(hp.quniform('n_estimators', 50, 1000, 1)),
        'learning_rate': hp.quniform('eta', 0.025, 0.5, 0.025),
        'max_depth': scope.int(hp.quniform("x_max_depth", 2, 10, 2)),
        'min_child_weight' : scope.int(hp.quniform('min_child_weight', 2, 6, 1)),
        'subsample' : hp.quniform('subsample', 0.7, 1.0, 0.1),
        'gamma' : hp.quniform('gamma',0.1,0.5,0.1)
    }     
def objective(space):
    xgb_dtc.set_params(**space)
    shuffle = KFold(n_splits=5, shuffle=True)
    score = cross_val_score(xgb_dtc, X_train, y_train_dtc, cv=shuffle, scoring='r2', n_jobs=-1)
    return 1-score.mean()
trials = Trials()
best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=100,
            trials=trials)
best_params = space_eval(space, best_dtc)
xgb_dtc.set_params(**best_params)
xgb_dtc.fit(X_train, y_train_dtc)
y_predict_dtc=xgb_dtc.predict(X_test)

print(best)  
```
*XGB* <br>
![alt]({{ site.url }}{{ site.baseurl }}/images/02_well/xgb.jpg)

I used the shap library to examin the effects of individual features on the predictive result. The top 3 important features are CAL, CNC, and ZEN. The relationship between the CNC and ZDEN and the predictive value are pretty straight forward. The larger CNC predicted larger target value, while smaller ZDEN predicted larger target value. CAL usually impact the predictive towards smaller value, but relationship is more complicated.

From the XGB predictive result profile, I noticed that the predictive result is relatively close to the true value while the errors occurred mostly in predicting higher values. I selected two predictive values, one for the low and one for the higher to examine the impacts of different features.
```python
import shap
# load JS visualization code to notebook
shap.initjs()
# explain the model's predictions using SHAP
# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)
explainer = shap.TreeExplainer(xgb_dts)
shap_values = explainer.shap_values(df2_x_scl)
shap.summary_plot(shap_values, df2_x)
```
*feature effect* <br>
![alt]({{ site.url }}{{ site.baseurl }}/images/02_well/factor1.jpg)

PE and CNC are trying to push the value towards higher side, whereas CAL is pushing it towards lower side, hence final value is much lower than the actual one ( 299 ).
```python
high_dts_ix = df2_y.loc[df2_y['DTS  ']>300,:].index.to_list()
i = high_dts_ix[0]
# visualize a specific prediction's explanation (use matplotlib=True to avoid Javascript)
shap.force_plot(explainer.expected_value,shap_values[i,:], df2_x.iloc[i,:],matplotlib=True)
print(df2_y.iloc[i,:]['DTS  '])
```
*explanation for prediction of larger value* <br>
![alt]({{ site.url }}{{ site.baseurl }}/images/02_well/factor3.jpg)

# Some Extra Thoughts
The DTC and DTS values vary a lot in different type of rocks. Use the petrophysics-based zonation for the train and test dataset based on GR, ZDEN and CNC. The predictive models were trained and the targets were predicted for each zone. The lithology classification is based on the deterministic method. Using the same gradient boosting method improved the predictive accuracy, and the RMSE score reduced to 15.20. However, when I tried some data-drive classification or clustering method, I have not observed the same improvement. The professional domain knowledge is still the key for feature engineering and more accurate result. But this project can be further explored to use more data-driven method to bring same quality result.

*lithology classification based on deterministic method* <br>
![alt]({{ site.url }}{{ site.baseurl }}/images/02_well/class.jpg)


# Summary
Among all the tested models, the XGboost regression model delivered the current best result with RMSE 17.58399. Models are able to predict DTC quite closely, as well as the DTS up to 200. The majority errors occurred in predicting higher values.
With some help of the classification based on a deterministic method, the prediction accuracy improved, while the data-driven clustering method struggled to deliver the same improvement. I did not dive deep in the data-driven lithology classification method, but I think it's potential to build a more data-driven model.
